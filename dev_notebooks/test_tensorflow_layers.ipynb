{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "from nems import Model\n",
    "from nems.tf.model import build_model\n",
    "from nems.layers import FIR\n",
    "\n",
    "fir = FIR(shape=(1, 15), name='fir')\n",
    "fir_tf = fir.as_tensorflow_layer()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 4\n",
    "n_out = 4\n",
    "\n",
    "x = np.random.rand(10, 1000, 18)  # 10 stimuli (as spectrogram) 1000 bins each\n",
    "y = np.random.rand(10, 1000, n_out)   # 10 corresponding responses\n",
    "data = {'input': x}#, 'target': y}  # only include inputs (stim, state, etc)\n",
    "\n",
    "# TODO: this doesn't work right now b/c fir assumes it should be 4x15.\n",
    "#       Old NEMS got around that by multiplying first and last dims, but\n",
    "#       I was trying to avoid that...\n",
    "nems_model = Model.from_keywords(f'wc.18x1x{n_units}-fir.1x15x{n_units}')\n",
    "fir_tf = nems_model['fir'].as_tensorflow_layer()()\n",
    "# Check that input shape gets evaluated correctly\n",
    "fir_tf.call(keras.Input(shape=(1000, 1, 4), name='test', dtype='float32'))\n",
    "# TODO: not sure what the Lambda layer warning is about, but it's not happening\n",
    "#       during the actual model evaluation so maybe nothing to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_layers = [layer.as_tensorflow_layer()() for layer in nems_model.layers]\n",
    "tf_model = build_model(\n",
    "    nems_model, tf_layers, data, eval_kwargs={'input_name': 'input'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move NEMS Model.__repr__ to Model.summary() instead of Model.__str__?\n",
    "#       (or maybe both, but still switch __repr__ to compact version)\n",
    "tf_model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss={\n",
    "        'fir': keras.losses.MeanSquaredError()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.fit(\n",
    "    data, {'fir': y}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.predict(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.layers[2].weights_to_values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in tf_model.layers:\n",
    "    print(layer.weights_to_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ShapeError",
     "evalue": "Shape mismatch in WeightChannels.evaluate.\ninput has shape: (1, 1000, 18)\ncoefficients has shape: (18, 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\layers\\weight_channels.py:96\u001b[0m, in \u001b[0;36mWeightChannels.evaluate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mtensordot(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefficients, axes\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Check for dimension swap, to give more informative error message.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m#if 'mismatch in its core dimension' in str(e):  # for @\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\mkltest\\lib\\site-packages\\numpy\\core\\numeric.py:1110\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m equal:\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mshape-mismatch for sum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1112\u001b[0m \u001b[39m# Move the axes to sum over to the end of \"a\"\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[39m# and to the front of \"b\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shape-mismatch for sum",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mShapeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\nems-lite\\dev_notebooks\\test_tensorflow_layers.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/nems-lite/dev_notebooks/test_tensorflow_layers.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# with tf.device(\"/CPU:0\"):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/nems-lite/dev_notebooks/test_tensorflow_layers.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39m/GPU:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/nems-lite/dev_notebooks/test_tensorflow_layers.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     tf_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit({\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39minput\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m: target}, backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/nems-lite/dev_notebooks/test_tensorflow_layers.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                          fitter_options\u001b[39m=\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:668\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, input, target, target_name, backend, cost_function, fitter_options, backend_options, **eval_kwargs)\u001b[0m\n\u001b[0;32m    660\u001b[0m     cost_function \u001b[39m=\u001b[39m get_metric(cost_function)\n\u001b[0;32m    662\u001b[0m \u001b[39m# TODO: Answer: enforce always having the batch/sample dimension, but\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m#       expect data w/o it by default. I.e. prepend a singleton dim\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39m#       unless user specifies batch=something\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[39m# Evaluate once prior to fetching backend, to ensure all DataMaps are\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m# up to date and include outputs.\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m    669\u001b[0m     \u001b[39minput\u001b[39m, use_existing_maps\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meval_kwargs\n\u001b[0;32m    670\u001b[0m     )\n\u001b[0;32m    672\u001b[0m \u001b[39m# TODO: replace with DataSet method?\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_data(\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meval_kwargs)\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:249\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, input, state, input_name, state_name, output_name, n, return_full_data, use_existing_maps, batch_size)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39melse\u001b[39;00m: n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[39m# Get data for the final layer only, to reduce memory use.\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m layer_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_layer_data(data_generator, n, n)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_full_data:\n\u001b[0;32m    252\u001b[0m     data \u001b[39m=\u001b[39m layer_data[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# output of final Layer._evaluate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:500\u001b[0m, in \u001b[0;36mModel.get_layer_data\u001b[1;34m(self, data_generator, first_index, last_index)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mif\u001b[39;00m last_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: last_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    499\u001b[0m subset \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mislice(data_generator, first_index, last_index)\n\u001b[1;32m--> 500\u001b[0m \u001b[39mreturn\u001b[39;00m [d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m subset]\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:500\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mif\u001b[39;00m last_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: last_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    499\u001b[0m subset \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mislice(data_generator, first_index, last_index)\n\u001b[1;32m--> 500\u001b[0m \u001b[39mreturn\u001b[39;00m [d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m subset]\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:445\u001b[0m, in \u001b[0;36mModel.generate_layer_data\u001b[1;34m(self, input, copy_data, use_existing_maps, **eval_kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_existing_maps:\n\u001b[0;32m    444\u001b[0m     layer\u001b[39m.\u001b[39mreset_map()\n\u001b[1;32m--> 445\u001b[0m a, k, o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_layer(layer, data)\n\u001b[0;32m    446\u001b[0m layer_data \u001b[39m=\u001b[39m {\n\u001b[0;32m    447\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m: n, \u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m: layer\u001b[39m.\u001b[39mname,\n\u001b[0;32m    448\u001b[0m     \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m: a, \u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m: k, \u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m: o\n\u001b[0;32m    449\u001b[0m     }\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m (max_n \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\models\\base.py:326\u001b[0m, in \u001b[0;36mModel._evaluate_layer\u001b[1;34m(self, layer, data)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39m\"\"\"Evaluates one Layer. Internal for `Model.generate_layer_data`.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mReturns\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m# Get input & output arrays\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m args, kwargs, output \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49m_evaluate(data)\n\u001b[0;32m    328\u001b[0m \u001b[39m# Save output (or don't) based on Layer.DataMap.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m# data_keys is always a list, but output might be a list or one array.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m data_keys \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mdata_map\u001b[39m.\u001b[39mout\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\layers\\base\\layer.py:341\u001b[0m, in \u001b[0;36mLayer._evaluate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39m\"\"\"Get inputs from `data`, evaluate them, and update `Layer.data_map`.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m \n\u001b[0;32m    339\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    340\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_map\u001b[39m.\u001b[39mget_inputs(data)\n\u001b[1;32m--> 341\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m \u001b[39m# Add singleton channel axis to each array if missing.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\code\\nems-lite\\nems\\layers\\weight_channels.py:101\u001b[0m, in \u001b[0;36mWeightChannels.evaluate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Check for dimension swap, to give more informative error message.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m#if 'mismatch in its core dimension' in str(e):  # for @\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mshape-mismatch for sum\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n\u001b[1;32m--> 101\u001b[0m         \u001b[39mraise\u001b[39;00m ShapeError(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape,\n\u001b[0;32m    102\u001b[0m                          coefficients\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefficients\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    103\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m         \u001b[39m# Otherwise let the original error through.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;31mShapeError\u001b[0m: Shape mismatch in WeightChannels.evaluate.\ninput has shape: (1, 1000, 18)\ncoefficients has shape: (18, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nems import Model\n",
    "\n",
    "input = np.random.rand(1, 1000, 18)  # 1 stimulus (as spectrogram) 1000 bins each\n",
    "target = np.random.rand(1, 1000, 1)   # 1 corresponding \"response\"\n",
    "model = Model.from_keywords('wc.18x2-fir.15x2-wc.1x1-relu.1')\n",
    "# model.freeze_layers('fir')\n",
    "# model.fit(input, target, backend='tf')  # TODO: arrays\n",
    "options = {'learning_rate': 1e2, 'epochs': 2}\n",
    "# with tf.device(\"/CPU:0\"):\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    tf_model = model.fit({'input': input, 'target': target}, backend='tf',\n",
    "                         fitter_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nems_prediction = model.evaluate(input[0, ...])['output']\n",
    "nems_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_prediction = tf_model.predict(input)[0,...]\n",
    "tf_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(nems_prediction, tf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mkltest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e04c614ea49137b728f9a88f47ebc62abd1dc770924cb0e56431acc2e9f8803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
